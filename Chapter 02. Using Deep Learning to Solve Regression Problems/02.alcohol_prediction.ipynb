{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Using deep neural networks for regression\n",
    "## 2.1. How to plan a machine learning problem\n",
    "* Khi xây dựng một deep neural network, hãy theo dõi các bước sau:\n",
    "  * Vạch ra vấn đề ta đang muốn giải quyết.\n",
    "  * Xác định input và output của model.\n",
    "  * Xác định cost function.\n",
    "  * Tạo một network.\n",
    "  * Đào tạo và hiệu chỉnh network.\n",
    "  \n",
    "## 2.2. Defining our example problem\n",
    "* Chúng ta sẽ sử dụng dataset ***wine quality*** [chất lượng rượu] [tại đây](https://archive.ics.uci.edu/ml/datasets/wine+quality) cho các bài toán cho đến khi tôi thông báo ta sẽ sử dụng một dataset mới.\n",
    "* Có tổng số 4898 observation trong dataset này. Con số này có vẻ khá lơn với bài toán regression cổ điển nhưng lại khá nhỏ cho deep neural network.\n",
    "* Có 9 đặc điểm hóa học đầu tiên mà ta cần dùng để dự đoán cho target variable (`alcohol`) và chúng đều là các continous variable. Vùng blue square _(bỏ cái `sulphates` đi, vẽ lộn)_ là independent variable và red square là dependent variable:<br>\n",
    "  ![](./images/02.00.png)\n",
    "\n",
    "## 2.3. Loading the dataset\n",
    "* Chúng ta cần load dữ liệu wine quality lên như sau:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "TRAIN_DATA = \"./data/train/train_data.csv\"\n",
    "VAL_DATA = \"./data/val/val_data.csv\"\n",
    "TEST_DATA = \"./data/test/test_data.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def load_data():\n",
    "    '''Load train, val and test datasets from disk.'''\n",
    "    train = pd.read_csv(TRAIN_DATA)\n",
    "    val = pd.read_csv(VAL_DATA)\n",
    "    test = pd.read_csv(TEST_DATA)\n",
    "    \n",
    "    '''Using sklearn's StandardScaler to scale our data to 0 mean and unit variance.'''\n",
    "    scaler = StandardScaler()\n",
    "    train = scaler.fit_transform(train)\n",
    "    val = scaler.transform(val)\n",
    "    test = scaler.transform(test)\n",
    "    \n",
    "    '''We will use a dict to keep all this data.'''\n",
    "    data = dict()\n",
    "    data['train_y'] = train[:, 10]\n",
    "    data['train_X'] = train[:, 0:9]\n",
    "    data['val_y'] = val[:, 10]\n",
    "    data['val_X'] = val[:, 0:9]\n",
    "    data['test_y'] = test[:, 10]\n",
    "    data['test_X'] = test[:, 0:9]\n",
    "    \n",
    "    '''Keep the `scaler` so we can unscale prediction in the future.'''\n",
    "    data['scaler'] = scaler\n",
    "    \n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4. Defining our cost function\n",
    "* Đối với các bài toán hồi quy, cost function hay được dùng là **Root Mean Squared Error - RMSE** và **Mean Absolute Error - MAE**. Ở đây chúng ta sẽ dùng MAE, công thức như sau:\n",
    "  $$\\mathrm{MAE} = \\dfrac{1}{n} \\sum_{j = 1}^n |y_j - \\widehat{y_j}|$$\n",
    "\n",
    "* Bạn cũng có thể sử dụng RMSE như sau:\n",
    "  $$\\mathrm{RMSE} = \\sqrt{\\dfrac{1}{n} \\sum_{j = 1}^n (y_j - \\widehat{y_j})^2}$$\n",
    "\n",
    "* Có thể bạn sẽ bối rối ụa hai cái này làm sao biết nên chọn cái nào:\n",
    "  * Trong trường hợp lỗi phân bố đều trên training data thì $\\mathrm{RMSE} = \\mathrm{MAE}$, ví dụ ta có 10 data point và 10 data point này đều có $|y_j - \\widehat{y_j}| = 5$.\n",
    "  * Còn nếu training data có outlier thì $\\mathrm{RMSE}$ sẽ **lớn hơn rất nhiều** so với $\\mathrm{MAE}$.\n",
    "* Không có một chuẩn nào đặt ra cho việc lựa chọn cost function, chủ yếu là do cảm nhận của bạn vào dataset. Nhưng về diễn giải, $\\mathrm{MAE}$ dễ hiểu hơn so với $\\mathrm{RMSE}$.\n",
    "\n",
    "# 3. Building and MLP in Keras\n",
    "* Một model của Keras là một tập hợp của các layer và chúng ta cần định nghĩa tập các layer này cho Keras hiểu.\n",
    "* Keras hiện có hai API để đào tạo model. Trong ví dụ này, chúng ta sẽ sử dụng **Functional API** - nó khá là dài dòng về mặt coding nhưng khả năng tùy biến của nó cao. Hầu hết các pro đều sài Functional API.\n",
    "\n",
    "* Model MLP của chúng ta sẽ cần:\n",
    "  * Một input layer.\n",
    "  * Một hidden layer.\n",
    "  * Một output layer.\n",
    "\n",
    "## 3.1. Input layer shape\n",
    "* Input của chúng ta là một matrix với số dòng là số lượng observation và số cột là số lượng feature của dataset. Vậy input matrix của chúng ta có shape là $\\text{số lượng observation} \\times 10$.\n",
    "* Tuy nhiên, chúng ta không cần phải xác định chính xác shape của input matrix. TensorFlow và Keras cho phép chúng ta định nghĩa giá trị `None` cho biến **placeholder** và chúng ta hoàn toàn có thể định nghĩa lại biến placeholder này về sau.\n",
    "\n",
    "## 3.2. Hidden layer shape\n",
    "* Hidden layer của chúng ta sẽ có 32 neutron. Tại thời điểm này chúng ta không thể biết chính xác chúng ta cần bao nhiêu neutron cho network vì đây là một hyperparameter và ta cần hiệu chỉnh nó về sau. Việc xác định kiến trúc của một network là một vấn đề mở rộng trong deep learning.\n",
    "* Vì chúng ta có 32 neutron trong hidden layer, input layer của chúng ta gồm 10 feature nên shape của hidden layer là (10, 32).\n",
    "\n",
    "## 3.3. Output layer shape\n",
    "* Output layer của chúng ta sẽ bao gồm duy nhất một neuron, nó nhận vào 32 neutron của hidden layer như là input của nó và tiến hành dự đoán ra một giá trị $\\widehat{y}$ duy nhất cho từng data point.\n",
    "* Vậy model MLP của chúng ta sẽ trông thế này:<br>\n",
    "  <center>\n",
    "\n",
    "    ![](./images/02.01.png)\n",
    "\n",
    "  </center>\n",
    "  \n",
    "## 3.4. Neural network architecture\n",
    "* Bây giờ chúng ta sẽ định nghĩa input và output."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def build_network(input_features=None):\n",
    "    inputs = Input(shape=(input_features,), name='input') # input layer\n",
    "    x = Dense(32, activation='relu', name='hidden')(inputs) # hidden layer với 32 neuron\n",
    "    prediction = Dense(1, activation='linear', name='final')(x) # output layer với 1 neuron\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=prediction) # build model với input và output\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error') # biên dịch model với adam optimizer và loss mà MAE\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Hãy khám phá hàm trên:\n",
    "  * Dòng code số 4 ta dùng activation function là `linear`, điều này cũng giống như việc chúng ta không sử dụng bất kì activation function nào, đây là những gì mà chúng ta muốn cho bài toán hồi quy\n",
    "  * Dòng code số 6, ta định nghĩa đâu là input layer và đâu là output layer cho `Model` object.\n",
    "  * Dòng code số 7, ta định nghĩa Adam optimizer cho `optimizer` và MAE cho `loss` function.\n",
    "* Bây giờ chúng ta có thể gọi hàm `build_model()` để xây dựng một neural network như sau:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = build_network(input_features=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Giả sử bây giờ ta muốn hiệu chỉnh các hyperparameter của Adam optimizer thì ta có thể làm như sau:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from keras.optimizers import adam_v2\n",
    "\n",
    "def build_network_with_adam_hyperparams(input_features=None):\n",
    "    inputs = Input(shape=(input_features,), name='input') # input layer\n",
    "    x = Dense(32, activation='relu', name='hidden')(inputs) # hidden layer với 32 neuron\n",
    "    prediction = Dense(1, activation='linear', name='final')(x) # output layer với 1 neuron\n",
    "    \n",
    "    adam_optimizer = adam_v2.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=prediction) # build model với input và output\n",
    "    model.compile(optimizer=adam_optimizer, loss='mean_absolute_error') # biên dịch model với adam optimizer và loss mà MAE\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5. Training the Keras model\n",
    "* Bây giờ chúng ta sẽ load data lên bằng hàm `load_data()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "data = load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Bây giờ chúng ta sẽ lấy số feature từ tập training data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "input_features = data['train_X'].shape[1]\n",
    "\n",
    "print(\"Có {} features bên trong training fata.\".format(input_features))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Có 9 features bên trong training fata.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Bây giờ sẽ build model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "'''Load model architecture'''\n",
    "model = build_network(input_features=input_features)\n",
    "\n",
    "'''Fit model with training data and hyperparameters'''\n",
    "model.fit(x=data['train_X'], y=data['train_y'],\n",
    "          batch_size=32, epochs=200, verbose=1, \n",
    "          validation_data=(data['val_X'], data['val_y']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "123/123 [==============================] - 2s 3ms/step - loss: 0.6212 - val_loss: 0.4836\n",
      "Epoch 2/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.4235 - val_loss: 0.3615\n",
      "Epoch 3/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.2890\n",
      "Epoch 4/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.2789 - val_loss: 0.2607\n",
      "Epoch 5/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2495 - val_loss: 0.2456\n",
      "Epoch 6/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2346 - val_loss: 0.2460\n",
      "Epoch 7/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2292 - val_loss: 0.2410\n",
      "Epoch 8/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2240 - val_loss: 0.2367\n",
      "Epoch 9/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2216 - val_loss: 0.2384\n",
      "Epoch 10/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2191 - val_loss: 0.2387\n",
      "Epoch 11/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2184 - val_loss: 0.2324\n",
      "Epoch 12/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2167 - val_loss: 0.2325\n",
      "Epoch 13/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2150 - val_loss: 0.2362\n",
      "Epoch 14/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2144 - val_loss: 0.2320\n",
      "Epoch 15/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2133 - val_loss: 0.2336\n",
      "Epoch 16/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2126 - val_loss: 0.2289\n",
      "Epoch 17/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2114 - val_loss: 0.2291\n",
      "Epoch 18/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2109 - val_loss: 0.2279\n",
      "Epoch 19/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2111 - val_loss: 0.2303\n",
      "Epoch 20/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2105 - val_loss: 0.2305\n",
      "Epoch 21/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2102 - val_loss: 0.2297\n",
      "Epoch 22/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2085 - val_loss: 0.2273\n",
      "Epoch 23/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2085 - val_loss: 0.2262\n",
      "Epoch 24/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2083 - val_loss: 0.2272\n",
      "Epoch 25/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2076 - val_loss: 0.2287\n",
      "Epoch 26/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2075 - val_loss: 0.2261\n",
      "Epoch 27/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.2076 - val_loss: 0.2260\n",
      "Epoch 28/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.2056 - val_loss: 0.2255\n",
      "Epoch 29/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.2056 - val_loss: 0.2276\n",
      "Epoch 30/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2057 - val_loss: 0.2238\n",
      "Epoch 31/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2058 - val_loss: 0.2284\n",
      "Epoch 32/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2058 - val_loss: 0.2278\n",
      "Epoch 33/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.2051 - val_loss: 0.2227\n",
      "Epoch 34/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2046 - val_loss: 0.2211\n",
      "Epoch 35/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2033 - val_loss: 0.2274\n",
      "Epoch 36/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2041 - val_loss: 0.2232\n",
      "Epoch 37/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2038 - val_loss: 0.2229\n",
      "Epoch 38/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2039 - val_loss: 0.2228\n",
      "Epoch 39/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2025 - val_loss: 0.2241\n",
      "Epoch 40/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2041 - val_loss: 0.2232\n",
      "Epoch 41/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2034 - val_loss: 0.2251\n",
      "Epoch 42/200\n",
      "123/123 [==============================] - 0s 959us/step - loss: 0.2035 - val_loss: 0.2229\n",
      "Epoch 43/200\n",
      "123/123 [==============================] - 0s 957us/step - loss: 0.2026 - val_loss: 0.2237\n",
      "Epoch 44/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.2029 - val_loss: 0.2223\n",
      "Epoch 45/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.2025 - val_loss: 0.2217\n",
      "Epoch 46/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2014 - val_loss: 0.2206\n",
      "Epoch 47/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.2006 - val_loss: 0.2216\n",
      "Epoch 48/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2026 - val_loss: 0.2226\n",
      "Epoch 49/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2015 - val_loss: 0.2197\n",
      "Epoch 50/200\n",
      "123/123 [==============================] - 0s 959us/step - loss: 0.2015 - val_loss: 0.2227\n",
      "Epoch 51/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2002 - val_loss: 0.2229\n",
      "Epoch 52/200\n",
      "123/123 [==============================] - 0s 980us/step - loss: 0.2006 - val_loss: 0.2213\n",
      "Epoch 53/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2004 - val_loss: 0.2206\n",
      "Epoch 54/200\n",
      "123/123 [==============================] - 0s 963us/step - loss: 0.2001 - val_loss: 0.2195\n",
      "Epoch 55/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1997 - val_loss: 0.2207\n",
      "Epoch 56/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1999 - val_loss: 0.2208\n",
      "Epoch 57/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2011 - val_loss: 0.2203\n",
      "Epoch 58/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1987 - val_loss: 0.2193\n",
      "Epoch 59/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1996 - val_loss: 0.2198\n",
      "Epoch 60/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.2001 - val_loss: 0.2201\n",
      "Epoch 61/200\n",
      "123/123 [==============================] - 0s 966us/step - loss: 0.1995 - val_loss: 0.2187\n",
      "Epoch 62/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1997 - val_loss: 0.2201\n",
      "Epoch 63/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1996 - val_loss: 0.2223\n",
      "Epoch 64/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1992 - val_loss: 0.2181\n",
      "Epoch 65/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1989 - val_loss: 0.2181\n",
      "Epoch 66/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1987 - val_loss: 0.2153\n",
      "Epoch 67/200\n",
      "123/123 [==============================] - 0s 945us/step - loss: 0.2000 - val_loss: 0.2209\n",
      "Epoch 68/200\n",
      "123/123 [==============================] - 0s 949us/step - loss: 0.1985 - val_loss: 0.2169\n",
      "Epoch 69/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1986 - val_loss: 0.2191\n",
      "Epoch 70/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1975 - val_loss: 0.2170\n",
      "Epoch 71/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1989 - val_loss: 0.2169\n",
      "Epoch 72/200\n",
      "123/123 [==============================] - 0s 947us/step - loss: 0.1981 - val_loss: 0.2174\n",
      "Epoch 73/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1970 - val_loss: 0.2143\n",
      "Epoch 74/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1972 - val_loss: 0.2185\n",
      "Epoch 75/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1975 - val_loss: 0.2184\n",
      "Epoch 76/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1980 - val_loss: 0.2160\n",
      "Epoch 77/200\n",
      "123/123 [==============================] - 0s 972us/step - loss: 0.1978 - val_loss: 0.2170\n",
      "Epoch 78/200\n",
      "123/123 [==============================] - 0s 968us/step - loss: 0.1963 - val_loss: 0.2164\n",
      "Epoch 79/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1969 - val_loss: 0.2150\n",
      "Epoch 80/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1974 - val_loss: 0.2200\n",
      "Epoch 81/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1966 - val_loss: 0.2154\n",
      "Epoch 82/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1962 - val_loss: 0.2148\n",
      "Epoch 83/200\n",
      "123/123 [==============================] - 0s 985us/step - loss: 0.1970 - val_loss: 0.2139\n",
      "Epoch 84/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1965 - val_loss: 0.2179\n",
      "Epoch 85/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1958 - val_loss: 0.2172\n",
      "Epoch 86/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1968 - val_loss: 0.2154\n",
      "Epoch 87/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1968 - val_loss: 0.2160\n",
      "Epoch 88/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1964 - val_loss: 0.2160\n",
      "Epoch 89/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1968 - val_loss: 0.2188\n",
      "Epoch 90/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1962 - val_loss: 0.2159\n",
      "Epoch 91/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1961 - val_loss: 0.2124\n",
      "Epoch 92/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1972 - val_loss: 0.2154\n",
      "Epoch 93/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1959 - val_loss: 0.2164\n",
      "Epoch 94/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1959 - val_loss: 0.2140\n",
      "Epoch 95/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1954 - val_loss: 0.2142\n",
      "Epoch 96/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1967 - val_loss: 0.2182\n",
      "Epoch 97/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1960 - val_loss: 0.2180\n",
      "Epoch 98/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1957 - val_loss: 0.2158\n",
      "Epoch 99/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1954 - val_loss: 0.2181\n",
      "Epoch 100/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1951 - val_loss: 0.2180\n",
      "Epoch 101/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1951 - val_loss: 0.2150\n",
      "Epoch 102/200\n",
      "123/123 [==============================] - 0s 955us/step - loss: 0.1946 - val_loss: 0.2196\n",
      "Epoch 103/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1948 - val_loss: 0.2179\n",
      "Epoch 104/200\n",
      "123/123 [==============================] - 0s 966us/step - loss: 0.1947 - val_loss: 0.2158\n",
      "Epoch 105/200\n",
      "123/123 [==============================] - 0s 957us/step - loss: 0.1956 - val_loss: 0.2149\n",
      "Epoch 106/200\n",
      "123/123 [==============================] - 0s 956us/step - loss: 0.1955 - val_loss: 0.2147\n",
      "Epoch 107/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1954 - val_loss: 0.2181\n",
      "Epoch 108/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1944 - val_loss: 0.2185\n",
      "Epoch 109/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1946 - val_loss: 0.2177\n",
      "Epoch 110/200\n",
      "123/123 [==============================] - 0s 954us/step - loss: 0.1952 - val_loss: 0.2177\n",
      "Epoch 111/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1948 - val_loss: 0.2182\n",
      "Epoch 112/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1941 - val_loss: 0.2213\n",
      "Epoch 113/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1944 - val_loss: 0.2173\n",
      "Epoch 114/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1957 - val_loss: 0.2166\n",
      "Epoch 115/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1941 - val_loss: 0.2136\n",
      "Epoch 116/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1938 - val_loss: 0.2167\n",
      "Epoch 117/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1947 - val_loss: 0.2188\n",
      "Epoch 118/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1942 - val_loss: 0.2171\n",
      "Epoch 119/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1943 - val_loss: 0.2172\n",
      "Epoch 120/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1935 - val_loss: 0.2151\n",
      "Epoch 121/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1945 - val_loss: 0.2154\n",
      "Epoch 122/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1939 - val_loss: 0.2225\n",
      "Epoch 123/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1947 - val_loss: 0.2183\n",
      "Epoch 124/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1932 - val_loss: 0.2189\n",
      "Epoch 125/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1940 - val_loss: 0.2196\n",
      "Epoch 126/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1939 - val_loss: 0.2157\n",
      "Epoch 127/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1929 - val_loss: 0.2177\n",
      "Epoch 128/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1942 - val_loss: 0.2172\n",
      "Epoch 129/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1930 - val_loss: 0.2176\n",
      "Epoch 130/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1929 - val_loss: 0.2161\n",
      "Epoch 131/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1942 - val_loss: 0.2164\n",
      "Epoch 132/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1940 - val_loss: 0.2175\n",
      "Epoch 133/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1927 - val_loss: 0.2205\n",
      "Epoch 134/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1943 - val_loss: 0.2185\n",
      "Epoch 135/200\n",
      "123/123 [==============================] - 0s 968us/step - loss: 0.1936 - val_loss: 0.2196\n",
      "Epoch 136/200\n",
      "123/123 [==============================] - 0s 977us/step - loss: 0.1934 - val_loss: 0.2210\n",
      "Epoch 137/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1932 - val_loss: 0.2201\n",
      "Epoch 138/200\n",
      "123/123 [==============================] - 0s 961us/step - loss: 0.1930 - val_loss: 0.2217\n",
      "Epoch 139/200\n",
      "123/123 [==============================] - 0s 948us/step - loss: 0.1938 - val_loss: 0.2179\n",
      "Epoch 140/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1928 - val_loss: 0.2210\n",
      "Epoch 141/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1921 - val_loss: 0.2185\n",
      "Epoch 142/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1923 - val_loss: 0.2144\n",
      "Epoch 143/200\n",
      "123/123 [==============================] - 0s 942us/step - loss: 0.1939 - val_loss: 0.2225\n",
      "Epoch 144/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1928 - val_loss: 0.2138\n",
      "Epoch 145/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1927 - val_loss: 0.2194\n",
      "Epoch 146/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1928 - val_loss: 0.2184\n",
      "Epoch 147/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1928 - val_loss: 0.2160\n",
      "Epoch 148/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1924 - val_loss: 0.2170\n",
      "Epoch 149/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1937 - val_loss: 0.2178\n",
      "Epoch 150/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1936 - val_loss: 0.2191\n",
      "Epoch 151/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1924 - val_loss: 0.2156\n",
      "Epoch 152/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1930 - val_loss: 0.2188\n",
      "Epoch 153/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1929 - val_loss: 0.2195\n",
      "Epoch 154/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1928 - val_loss: 0.2189\n",
      "Epoch 155/200\n",
      "123/123 [==============================] - 0s 959us/step - loss: 0.1926 - val_loss: 0.2155\n",
      "Epoch 156/200\n",
      "123/123 [==============================] - 0s 954us/step - loss: 0.1934 - val_loss: 0.2171\n",
      "Epoch 157/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1929 - val_loss: 0.2202\n",
      "Epoch 158/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1920 - val_loss: 0.2187\n",
      "Epoch 159/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1931 - val_loss: 0.2137\n",
      "Epoch 160/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1918 - val_loss: 0.2151\n",
      "Epoch 161/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1921 - val_loss: 0.2157\n",
      "Epoch 162/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1921 - val_loss: 0.2171\n",
      "Epoch 163/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1916 - val_loss: 0.2151\n",
      "Epoch 164/200\n",
      "123/123 [==============================] - 0s 930us/step - loss: 0.1931 - val_loss: 0.2153\n",
      "Epoch 165/200\n",
      "123/123 [==============================] - 0s 920us/step - loss: 0.1921 - val_loss: 0.2162\n",
      "Epoch 166/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1927 - val_loss: 0.2148\n",
      "Epoch 167/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1921 - val_loss: 0.2160\n",
      "Epoch 168/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1924 - val_loss: 0.2146\n",
      "Epoch 169/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1913 - val_loss: 0.2185\n",
      "Epoch 170/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1925 - val_loss: 0.2178\n",
      "Epoch 171/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1917 - val_loss: 0.2183\n",
      "Epoch 172/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1924 - val_loss: 0.2181\n",
      "Epoch 173/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1915 - val_loss: 0.2173\n",
      "Epoch 174/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1931 - val_loss: 0.2158\n",
      "Epoch 175/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1922 - val_loss: 0.2144\n",
      "Epoch 176/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1911 - val_loss: 0.2131\n",
      "Epoch 177/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1917 - val_loss: 0.2164\n",
      "Epoch 178/200\n",
      "123/123 [==============================] - 0s 965us/step - loss: 0.1916 - val_loss: 0.2153\n",
      "Epoch 179/200\n",
      "123/123 [==============================] - 0s 930us/step - loss: 0.1926 - val_loss: 0.2116\n",
      "Epoch 180/200\n",
      "123/123 [==============================] - 0s 962us/step - loss: 0.1918 - val_loss: 0.2157\n",
      "Epoch 181/200\n",
      "123/123 [==============================] - 0s 951us/step - loss: 0.1916 - val_loss: 0.2126\n",
      "Epoch 182/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1914 - val_loss: 0.2185\n",
      "Epoch 183/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1922 - val_loss: 0.2162\n",
      "Epoch 184/200\n",
      "123/123 [==============================] - 0s 931us/step - loss: 0.1914 - val_loss: 0.2157\n",
      "Epoch 185/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1916 - val_loss: 0.2137\n",
      "Epoch 186/200\n",
      "123/123 [==============================] - 0s 935us/step - loss: 0.1908 - val_loss: 0.2161\n",
      "Epoch 187/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1917 - val_loss: 0.2128\n",
      "Epoch 188/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1916 - val_loss: 0.2128\n",
      "Epoch 189/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1923 - val_loss: 0.2154\n",
      "Epoch 190/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1923 - val_loss: 0.2134\n",
      "Epoch 191/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1918 - val_loss: 0.2181\n",
      "Epoch 192/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1919 - val_loss: 0.2215\n",
      "Epoch 193/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1922 - val_loss: 0.2170\n",
      "Epoch 194/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1923 - val_loss: 0.2214\n",
      "Epoch 195/200\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.1911 - val_loss: 0.2147\n",
      "Epoch 196/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1909 - val_loss: 0.2178\n",
      "Epoch 197/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1916 - val_loss: 0.2177\n",
      "Epoch 198/200\n",
      "123/123 [==============================] - 0s 1ms/step - loss: 0.1907 - val_loss: 0.2169\n",
      "Epoch 199/200\n",
      "123/123 [==============================] - 0s 962us/step - loss: 0.1917 - val_loss: 0.2165\n",
      "Epoch 200/200\n",
      "123/123 [==============================] - 0s 950us/step - loss: 0.1901 - val_loss: 0.2112\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3da5693f98>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Bây giờ, hãy cũng điểm qua một vài đối số quan trọng của hàm Keras's `fit()`, cụ thể như sau:\n",
    "  * `batch_size`: Keras mặc định tham số này có giá trị là 32. Batch size số observation mà bạn muốn model lấy cho một lần đào tạo network. Ngoài ra có thể kham thảo thêm [bài viết này](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network) để hiểu rõ hơn về batch size, cũng như ưu điểm và nhược điểm của nó.\n",
    "  * `epochs`: là số lần mà model được nhìn thấy toàn bộ training data. Có thể tìm hiểu rõ hơn [tại đây](https://www.baeldung.com/cs/epoch-neural-networks).\n",
    "  * `validation_data`: ở đây, chúng ta chỉ cần cho model biết đâu là val data. Vào cuối mỗi epoch, Keras sẽ thực hiện kiểm tra model trên val data và đưa ra kết quả bằng loss function (cộng thêm các độ đo khác mà ta chỉ định).\n",
    "    * Ngoài ra, chúng ta có thể sử dụng tham số `validation_split` bằng một giá trị float dùng để chỉ định phần trăm từ tập training data mà ta muốn tách ra để làm val data cho model.\n",
    "  * `verbose`: tham số này không liên quan gì đến quá trình đào tạo network, nó chỉ đơn giản là dùng để hiển thị các thông tin trong quá trình đào tạo model. Nếu ta đặt là 1 nó sẽ xuất ra một thanh tiến trình hiển thị trạng thái đào tạo model qua từng epoch, đồng thời nó xuất ra cả thông tin về thời gian đào tạo qua từng epoch, giá trị của loss function. Nếu đặt bằng 2 thì nó sẽ chỉ xuất ra các giá trị loss function. Còn nếu đặt là 0 thì nó không xuất gì hết.\n",
    "* Ta có thể in ra thông tin tổng kết của model như sau:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 9)]               0         \n",
      "_________________________________________________________________\n",
      "hidden (Dense)               (None, 32)                320       \n",
      "_________________________________________________________________\n",
      "final (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 353\n",
      "Trainable params: 353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* ...\n",
    "  * Ở đây, cột `Param #` cho ta biết số lượng learnable parameters của từng layer.\n",
    "  * `Output Shape` cho biết hình dạng output đầu ra của từng layer. \n",
    "* Bây giờ, chúng ta sẽ lưu lại model để có thể tái sử dụng vào dự đoán sau này."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model.save(\"./models/wine_quality/regression_model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/wine_quality/regression_model/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Chúng ta load lại model sau này để sử dụng bằng hàm sau:\n",
    "  ~~~python\n",
    "  model = model.load(\"./models/wine_quality/regression_model\")\n",
    "  ~~~\n",
    "\n",
    "## 3.6. Measuring the performance of our model\n",
    "* Chúng ta đã đào tạo xong model của mình, chúng ta có thể dùng model để dự đoán như sau:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "test_pred = model.predict(data['test_X'])\n",
    "df = pd.DataFrame({\n",
    "    \"Test Actual Value\": data['test_y'],\n",
    "    \"Test Prediction Value\": test_pred.ravel()\n",
    "})\n",
    "\n",
    "print(\"Shape: {}\".format(df.shape))\n",
    "df.head(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape: (490, 2)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Actual Value</th>\n",
       "      <th>Test Prediction Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.092478</td>\n",
       "      <td>-0.343986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.315172</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.560018</td>\n",
       "      <td>-1.132815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.396702</td>\n",
       "      <td>0.591061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.842601</td>\n",
       "      <td>2.280361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.092478</td>\n",
       "      <td>0.128674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.559762</td>\n",
       "      <td>0.647970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.885881</td>\n",
       "      <td>0.721770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.989308</td>\n",
       "      <td>-1.117429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.663188</td>\n",
       "      <td>-0.841912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Test Actual Value  Test Prediction Value\n",
       "0          -0.092478              -0.343986\n",
       "1           0.315172               0.044300\n",
       "2          -1.560018              -1.132815\n",
       "3           0.396702               0.591061\n",
       "4           2.842601               2.280361\n",
       "5          -0.092478               0.128674\n",
       "6           0.559762               0.647970\n",
       "7           0.885881               0.721770\n",
       "8          -0.989308              -1.117429\n",
       "9          -0.663188              -0.841912"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Ta cũng có thể evaluation model như sau:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae_train = mean_absolute_error(data[\"train_y\"], model.predict(data[\"train_X\"]))\n",
    "mae_val = mean_absolute_error(data[\"val_y\"], model.predict(data[\"val_X\"]))\n",
    "mae_test = mean_absolute_error(data[\"test_y\"], model.predict(data[\"test_X\"]))\n",
    "\n",
    "print(\"Model Train MAE: {}\".format(mae_train))\n",
    "print(\"Model Val MAE: {}\".format(mae_val))\n",
    "print(\"Model Test MAE: {}\".format(mae_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Train MAE: 0.19497187468041116\n",
      "Model Val MAE: 0.2112075187495282\n",
      "Model Test MAE: 0.21319993850457947\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Nhận xét**\n",
    "> * Ta thấy rằng, MAE trên training data và test data lần lượt là 0.19 và 0.21, điều này chứng tỏ model ta không bị overfitting vì hai giá trị lỗi này khá gần nhau.\n",
    "> * Trong trường hợp MAE trên training data quá nhỏ nhưng trên test data thì lại quá lờn thì overfitting xảy ra. Tức lúc này model của ta có thể bị bias error, khi điều này xảy ra, chúng ta có thể thử thêm nhiều layer hơn, tăng số lượng neuron, hoặc cả hai. Chúng ta sẽ tìm hiểu kĩ hơn điều này sau.\n",
    "\n",
    "* Ta cũng có thể trực quan hóa sự khác biệt giữa actual value và prediction value trên test data như sau"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "y_hat = model.predict(data['test_X'])\n",
    "\n",
    "plt.title(\"Predicted Distribution vs Actual\")\n",
    "sns.distplot(y_hat.flatten(), label='y_hat')\n",
    "sns.distplot(data['test_y'], label='y_true')\n",
    "plt.legend();"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Chúng ta có thể lưu hành trên lại nếu thêm dòng code này vào ngay sau `plt.legend()`:\n",
    "  ```python\n",
    "  plt.savefig(\"pred_dist.jpg\")\n",
    "  ```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('python3.6': conda)"
  },
  "interpreter": {
   "hash": "2cade657992b47716e26d0a9b1443bfbca37741f9a577328e2d148cb3e78348d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}